# -*- coding: utf-8 -*-
"""Logistic_Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iGA13VTHRDn-WL2BDSDQvGaIROxLdN6i
"""

import seaborn as sns
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

!pip install seaborn
from google.colab import files
uploaded = files.upload()

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv("CarPrice_Assignment.csv")  # update path if needed
df.head()

print(df.shape)
print(df.info())
print(df.describe())

df = pd.read_csv("CarPrice_Assignment.csv")  # update path if needed

df = df[['enginesize', 'price']]
df.head()

X = df[['enginesize']]   # input feature
y = df['price']          # output variable

plt.figure(figsize=(8,5))
plt.scatter(X, y, color='green')
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Engine Size vs Car Price")
plt.show()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

y_pred_linear = linear_model.predict(X_test)

print("Linear RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_linear)))
print("Linear R2:", r2_score(y_test, y_pred_linear))

"""Polynomial_Regression"""

poly = PolynomialFeatures(degree=4)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)

y_pred_poly = poly_model.predict(X_test_poly)

print("Polynomial RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_poly)))
print("Polynomial R2:", r2_score(y_test, y_pred_poly))
print("polynomial mse:", np.sqrt(mean_squared_error(y_test,y_pred_poly)))

X_range = np.linspace(X.min(), X.max(), 300).reshape(-1,1)

# Linear line
y_linear_curve = linear_model.predict(X_range)

# Polynomial curve
X_range_poly = poly.transform(X_range)
y_poly_curve = poly_model.predict(X_range_poly)
plt.figure(figsize=(8,5))
plt.scatter(X, y, color='gray', alpha=0.5)
plt.plot(X_range, y_linear_curve, color='red', label='Linear Model')
plt.plot(X_range, y_poly_curve, color='green', label='Polynomial Model (degree=4)')
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Linear vs Polynomial Regression")
plt.legend()
plt.show()

engine_200 = np.array([[200]])

# Linear prediction
price_linear_200 = linear_model.predict(engine_200)

# Polynomial prediction
engine_200_poly = poly.transform(engine_200)
price_poly_200 = poly_model.predict(engine_200_poly)

print("Predicted price (Linear):", price_linear_200[0])
print("Predicted price (Polynomial):", price_poly_200[0])

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Example dataset: Study Hours vs. Pass/Fail (Binary Classification)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Study Hours
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
print("Predictions:", y_pred)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Plot decision boundary
X_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_prob = model.predict_proba(X_range)[:, 1]

plt.figure(figsize=(7,5))

# Scatter actual data
plt.scatter(X, y, color='blue', label='Actual Data')

# Logistic curve
plt.plot(X_range, y_prob, color='red', label='Logistic Curve')

plt.xlabel("Study Hours")
plt.ylabel("Probability of Passing")
plt.title("Logistic Regression Decision Boundary")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

ConfusionMatrixDisplay.from_predictions(
    y_test,
    y_pred,
    cmap="Blues",
    values_format="d"
)

plt.title("Confusion Matrix")
plt.show()

!pip install seaborn
from google.colab import files
uploaded = files.upload()

# ===============================
# 1. IMPORT LIBRARIES
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay
)

# ===============================
# 2. LOAD DATASET
# ===============================
df = pd.read_csv("heart.csv")
print(df.head())
print(df.info())

# ===============================
# 3. SELECT FEATURES & TARGET
# (Assumption: last column = target)
# ===============================
X = df.iloc[:, :-1]   # all columns except last
y = df.iloc[:, -1]    # last column

# ===============================
# 4. TRAIN–TEST SPLIT
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ===============================
# 5. TRAIN LOGISTIC REGRESSION
# ===============================
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# ===============================
# 6. PREDICTIONS
# ===============================
y_pred = model.predict(X_test)
print("Predictions:", y_pred)

# ===============================
# 7. ACCURACY & REPORT
# ===============================
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# ===============================
# 8. CONFUSION MATRIX (PRINT)
# ===============================
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# ===============================
# 9. CONFUSION MATRIX (PLOT)
# ===============================
ConfusionMatrixDisplay.from_predictions(
    y_test,
    y_pred,
    cmap="Greens",
    values_format="d"
)
plt.title("Confusion Matrix")
plt.show()

# ===============================
# 10. DECISION CURVE (ONLY IF 1 FEATURE)
# ===============================
if X.shape[1] == 1:
    X_range = np.linspace(X.min().values[0], X.max().values[0], 300).reshape(-1, 1)
    y_prob = model.predict_proba(X_range)[:, 1]

    plt.figure(figsize=(7,5))
    plt.scatter(X, y, color="blue", label="Actual Data")
    plt.plot(X_range, y_prob, color="red", label="Logistic Curve")
    plt.xlabel("Feature")
    plt.ylabel("Probability")
    plt.title("Logistic Regression Decision Boundary")
    plt.legend()
    plt.show()

!pip install seaborn
from google.colab import files
uploaded = files.upload()

# ===============================
# 1. IMPORT LIBRARIES
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay
)

# ===============================
# 2. LOAD DATASET
# ===============================
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

print(df.head())
print(df.info())

# ===============================
# 3. PREPROCESSING
# ===============================
# Drop 'customerID' as it's not a feature
df = df.drop('customerID', axis=1)

# Convert 'TotalCharges' to numeric, coercing errors to NaN
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
# Fill NaN values in 'TotalCharges' (e.g., with 0 or the mean/median)
df['TotalCharges'] = df['TotalCharges'].fillna(0) # or df['TotalCharges'].mean()

# Convert 'Churn' target variable to numerical (0 and 1)
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Define features (X) and target (y)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include='object').columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

# Create a column transformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ])

# ===============================
# 4. TRAIN–TEST SPLIT
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ===============================
# 5. PIPELINE (PREPROCESSING + LOGISTIC)
# ===============================
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', LogisticRegression(max_iter=1000))
])

pipeline.fit(X_train, y_train)

# ===============================
# 6. PREDICTIONS
# ===============================
y_pred = pipeline.predict(X_test)
print("Predictions:", y_pred)

# ===============================
# 7. ACCURACY & REPORT
# ===============================
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# ===============================
# 8. CONFUSION MATRIX (PRINT)
# ===============================
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# ===============================
# 9. CONFUSION MATRIX (PLOT)
# ===============================
ConfusionMatrixDisplay.from_predictions(
    y_test,
    y_pred,
    cmap="Blues",
    values_format="d"
)
plt.title("Confusion Matrix")
plt.show()

# ===============================
# 10. DECISION CURVE (ONLY IF 1 FEATURE)
# ===============================
# This block is commented out as it's not applicable for multi-feature models
# if X.shape[1] == 1:
#     X_range = np.linspace(X.min().values[0], X.max().values[0], 300).reshape(-1, 1)
#     y_prob = pipeline.predict_proba(X_range)[:, 1]

#     plt.figure(figsize=(7,5))
#     plt.scatter(X, y, color="blue", label="Actual Data")
#     plt.plot(X_range, y_prob, color="red", label="Logistic Curve")
#     plt.xlabel("Feature")
#     plt.ylabel("Probability")
#     plt.title("Logistic Regression Decision Boundary")
#     plt.legend()
#     plt.show()

